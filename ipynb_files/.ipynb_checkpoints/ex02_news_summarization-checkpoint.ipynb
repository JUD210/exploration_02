{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d4af85",
   "metadata": {},
   "source": [
    "# 3-4. 데이터 준비하기 | 15분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76388ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# mkdir -p ~/aiffel/news_summarization/data\n",
    "# ln -s ~/data/*.csv ~/aiffel/news_summarization/data\n",
    "# ▼ 심볼릭 링크에서 복사로 변경\n",
    "# cp ~/data/*.csv ~/aiffel/news_summarization/data\n",
    "# ▼ 폴더 구조에 맞게 변경\n",
    "!mkdir ../data\n",
    "!cp ~/data/*.csv ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf62e3d",
   "metadata": {},
   "source": [
    "이번 실습에서는 NLTK의 불용어(stopwords)를 사용할 거에요. NTLK와 NLTK 데이터셋이 설치되어 있지 않은 환경이라면 우선 NLTK를 설치하고 NTLK의 데이터셋을 다운로드해 주세요.\n",
    "\n",
    "NLTK는 Natural Language Toolkit의 축약어로 영어 기호, 통계, 자연어 처리를 위한 라이브러리에요. 이 NLTK에는 I, my, me, over, 조사, 접미사와 같이 문장에는 자주 등장하지만, 의미를 분석하고 요약하는 데는 거의 의미가 없는 100여개의 불용어가 미리 정리되어 있어요. 이를 이용해 다운로드한 리뷰 파일에서 불용어를 제거하는 작업을 진행할 예정이에요.\n",
    "\n",
    "NLTK 패키지에서 불용어 사전을 다운로드하고, 데이터 전처리를 위한 나머지 패키지도 함께 불러와 볼까요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcc13da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ea4e00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 100000\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.getenv(\"HOME\")+\"/aiffel/exploration_02/data/Reviews.csv\", nrows=100000)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65d119a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f839b",
   "metadata": {},
   "source": [
    "Q. 데이터프레임 data의 Text와 Summary 컬럼의 데이터만 남기는 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6060b2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82578</th>\n",
       "      <td>I make Aricot Zucchini jam with fresh zucchini...</td>\n",
       "      <td>Apricot jello used for making Apricot Zucchini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97046</th>\n",
       "      <td>My children brought me a bottle of this vanill...</td>\n",
       "      <td>great stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18932</th>\n",
       "      <td>My sister had this sent to me as a birthday pr...</td>\n",
       "      <td>Great Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35999</th>\n",
       "      <td>Sure, ordinary table salt is vastly cheaper bu...</td>\n",
       "      <td>Pricier but worth every dollar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47155</th>\n",
       "      <td>This food seriously is seriously gross looking...</td>\n",
       "      <td>Looks like vomit, the dogs love it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7470</th>\n",
       "      <td>SO CRUNCHIE GOOD!  HEALTHY ~ AND THE KIDS LOVE...</td>\n",
       "      <td>CRUNCHIE MUNCHIE GOOD!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5341</th>\n",
       "      <td>I am addicted to this gum. It tastes so close ...</td>\n",
       "      <td>Delicious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80151</th>\n",
       "      <td>Just what it says and perfect for grossing out...</td>\n",
       "      <td>Bugs in a bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29816</th>\n",
       "      <td>This tea is great hot or iced. I keep it a wor...</td>\n",
       "      <td>Refreshing mint tea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81348</th>\n",
       "      <td>OUR DOG LOVES THESE TREATS... SHE WILL DO ANYT...</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78045</th>\n",
       "      <td>Excellent popcorn.  Very tasty and cooks well ...</td>\n",
       "      <td>Delicious popcorn.  Prefer the purple variety ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78011</th>\n",
       "      <td>Both my puppy and I like the canned dog food. ...</td>\n",
       "      <td>Newman's Chicken Formula for Puppies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33860</th>\n",
       "      <td>natural balance shipment of high quality cat f...</td>\n",
       "      <td>high quality food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>My wife says that I can cook two things - omel...</td>\n",
       "      <td>Amish Country Lady Finger Popcorn - 2lb.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95624</th>\n",
       "      <td>We truly enjoy these chips, and were delighted...</td>\n",
       "      <td>Best Not-Baked Chip We've Tried!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "82578  I make Aricot Zucchini jam with fresh zucchini...   \n",
       "97046  My children brought me a bottle of this vanill...   \n",
       "18932  My sister had this sent to me as a birthday pr...   \n",
       "35999  Sure, ordinary table salt is vastly cheaper bu...   \n",
       "47155  This food seriously is seriously gross looking...   \n",
       "7470   SO CRUNCHIE GOOD!  HEALTHY ~ AND THE KIDS LOVE...   \n",
       "5341   I am addicted to this gum. It tastes so close ...   \n",
       "80151  Just what it says and perfect for grossing out...   \n",
       "29816  This tea is great hot or iced. I keep it a wor...   \n",
       "81348  OUR DOG LOVES THESE TREATS... SHE WILL DO ANYT...   \n",
       "78045  Excellent popcorn.  Very tasty and cooks well ...   \n",
       "78011  Both my puppy and I like the canned dog food. ...   \n",
       "33860  natural balance shipment of high quality cat f...   \n",
       "39994  My wife says that I can cook two things - omel...   \n",
       "95624  We truly enjoy these chips, and were delighted...   \n",
       "\n",
       "                                                 Summary  \n",
       "82578  Apricot jello used for making Apricot Zucchini...  \n",
       "97046                                        great stuff  \n",
       "18932                                      Great Present  \n",
       "35999                     Pricier but worth every dollar  \n",
       "47155                Looks like vomit, the dogs love it.  \n",
       "7470                              CRUNCHIE MUNCHIE GOOD!  \n",
       "5341                                           Delicious  \n",
       "80151                                      Bugs in a bag  \n",
       "29816                                Refreshing mint tea  \n",
       "81348                                                 AL  \n",
       "78045  Delicious popcorn.  Prefer the purple variety ...  \n",
       "78011               Newman's Chicken Formula for Puppies  \n",
       "33860                                  high quality food  \n",
       "39994           Amish Country Lady Finger Popcorn - 2lb.  \n",
       "95624                   Best Not-Baked Chip We've Tried!  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = [[YOUR CODE]]\n",
    "data = data[['Text', 'Summary']]\n",
    "data.head()\n",
    "\n",
    "#랜덤한 15개 샘플 출력\n",
    "data.sample(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e658a",
   "metadata": {},
   "source": [
    "2개의 열이 남았네요. Text 열의 내용을 요약한 것이 Summary 열이에요. 여기서는 인공 신경망을 통해 Text 시퀀스를 입력받으면, Summary 시퀀스를 예측하도록 인공 신경망을 훈련시킬 거예요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dc8565",
   "metadata": {},
   "source": [
    "# 3-5. 데이터 전처리하기 (1) 데이터 정리하기 | 35분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9bee3f",
   "metadata": {},
   "source": [
    "## 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdc5544b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 88426\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 72348\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1ffdd42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88426\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf96a6",
   "metadata": {},
   "source": [
    "중복이 제거되면서 샘플 수가 88,426개로 줄어들었어요. 그런데 만약 데이터 Null 값을 가지는 샘플이 있었다면, drop_duplicates()가 중복된 Null들을 지워주기는 하겠지만, 여전히 Null 값 한 개가 어딘가 남아있을 수 있어요. 데이터에 Null 값이 남아있는지 볼게요.\n",
    "\n",
    "데이터프레임에 Null 값이 있는지 확인하는 방법은 .isnull().sum()을 사용하면 알아볼 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6f3db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dec4fa",
   "metadata": {},
   "source": [
    "drop_duplicates()는 중복된 행만 제거하며, Null 값 자체는 중복으로 간주되지 않으므로 Null 값이 하나 남을 수 있습니다. 이 문제를 해결하려면 추가로 dropna()를 사용하여 Null 값을 처리하면 됩니다. 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93c71e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88425\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d846c1",
   "metadata": {},
   "source": [
    "## 텍스트 정규화와 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9987dcd2",
   "metadata": {},
   "source": [
    "살아남은 88,425개의 샘플에는 수많은 단어들이 있어요. 그런데 사실 그 단어들 중에서는 같은 의미인데도 다른 표현으로 쓰여 마치 다른 단어들처럼 간주되는 경우가 있어요.\n",
    "\n",
    "예를 들어서 it'll은 it will과 같고, mustn't과 must not은 사실 같은 표현이죠. 이런 경우 기계가 굳이 이들을 마치 다른 단어로 간주하게 해서 연산량을 늘리는 것보다는 기계 학습 전에 미리 같은 표현으로 통일시켜주는 것이 기계의 연산량을 줄일 수 있는 방법이에요.\n",
    "\n",
    "이러한 방법론을 텍스트 처리에서는 텍스트 정규화(text normalization) 라고 해요.\n",
    "\n",
    "여기서는 텍스트 정규화를 위한 사전(dictionary)을 아래와 같이 구성할 거예요. 이 사전은 아래의 링크에서 참고하여 만들었어요.\n",
    "\n",
    "정규화 사전 출처\n",
    "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ccdd213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285d6ab3",
   "metadata": {},
   "source": [
    "이제 정규화 준비까지 마쳤어요.\n",
    "\n",
    "하지만 아직 끝난 게 아니에요. 일반적으로 텍스트에는 자주 등장하지만 자연어 처리를 할 때 실질적으로 별 도움이 되지 않는 단어들이 존재해요. 이를 불용어(stopwords)라고 불러요. 때로는 불용어를 제거하는 것이 자연어 처리의 성능을 높이는 방법일 수 있어요. 여기서는 NLTK에서 제공하는 불용어 리스트를 참조해, 샘플에서 불용어를 제거할 거예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da98b9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea96dc",
   "metadata": {},
   "source": [
    "NLTK에서 미리 정의하여 제공하고 있는 불용어는 총 179개라는 것을 볼 수 있죠. 이를 사용하여 불용어를 제거할 거예요. 이 작업 외에도 모든 영어 문자는 소문자로 만들고, 섞여있는 html 태그를 제거하고, 정규 표현식을 통해 각종 특수문자를 제거해서 정말 필요한 내용만 잘 학습할 수 있도록 처리할 거예요.\n",
    "\n",
    "함수의 하단을 보면, NLTK를 이용해 불용어를 제거하는 파트가 있는데, 이는 Text 전처리 시에서만 호출하고 이미 상대적으로 문장 길이가 짧은 Summary 전처리할 때는 호출하지 않을 예정이에요. Abstractive한 문장 요약 결과문이 자연스러운 문장이 되려면 이 불용어들이 Summary에는 남아 있는 게 더 좋을 것 같습니다. 이 처리를 위해서 함수의 인자로 remove_stopwords를 추가하고, if문을 추가했어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbaa4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "156be095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  everything bought great infact ordered twice third ordered wasfor mother father\n",
      "summary: great way to start the day\n"
     ]
    }
   ],
   "source": [
    "temp_text = 'Everything I bought was great, infact I ordered twice and the third ordered was<br />for my mother and father.'\n",
    "temp_summary = 'Great way to start (or finish) the day!!!'\n",
    "\n",
    "print(\"text: \", preprocess_sentence(temp_text))\n",
    "print(\"summary:\", preprocess_sentence(temp_summary, False))  # 불용어를 제거하지 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c37fb3",
   "metadata": {},
   "source": [
    "결과를 보면 기본적으로 모든 알파벳이 소문자로 변환되고, <br />과 같은 html 태그가 제거되었죠. (or finish)와 같은 괄호로 묶였던 단어 시퀀스가 제거된 것도 확인할 수 있어요. 또한 특수문자가 제거되면서 영어만 남았어요.\n",
    "\n",
    "이제 함수가 잘 작동하는 것을 확인했으니, 훈련 데이터 전체에 대해서 전처리를 수행해볼게요. 이때, Text의 경우에는 불용어를 제거하고, Summary의 경우에는 불용어를 제거하지 않을 것이므로 따로 호출해서 진행해야 해요. 먼저 Text를 전처리하고, 결과를 확인하기 위해서 상위 5개의 줄을 출력해볼게요.\n",
    "\n",
    "Q. 위의 내용을 참고해서 훈련 데이터 전체의 Text 컬럼의 데이터를 전처리하는 코드를 작성하세요.(반복문 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "327a526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better', 'product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo', 'confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch', 'looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal', 'great taffy great price wide assortment yummy taffy delivery quick taffy lover deal']\n"
     ]
    }
   ],
   "source": [
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "clean_text = []\n",
    "\n",
    "# [[YOUR CODE]]\n",
    "for sentence in data['Text']:\n",
    "    processed_sentence = preprocess_sentence(sentence, remove_stopwords=True)  # 불용어 제거\n",
    "    clean_text.append(processed_sentence)\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f5451",
   "metadata": {},
   "source": [
    "이제 Summary에 대해서 전처리 함수를 호출해 줄 때는, 불용어 제거를 수행하지 않는다는 의미에서 두 번째 인자로 False를 넣어줄게요.\n",
    "\n",
    "Q. 위의 내용을 참고해서 훈련 데이터 전체의 Summary 컬럼의 데이터를 전처리하는 코드를 작성하세요.(반복문 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48460b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary 전처리 후 결과:  ['good quality dog food', 'not as advertised', 'delight says it all', 'cough medicine', 'great taffy']\n"
     ]
    }
   ],
   "source": [
    "# 전체 Summary 데이터에 대한 전처리 : 5분 이상 시간이 걸릴 수 있습니다. \n",
    "clean_summary = []\n",
    "\n",
    "# [[YOUR CODE]]\n",
    "for sentence in data['Summary']:\n",
    "    processed_sentence = preprocess_sentence(sentence, remove_stopwords=False)  # 불용어 제거하지 않음\n",
    "    clean_summary.append(processed_sentence)\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_summary[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53aab0",
   "metadata": {},
   "source": [
    "이렇게 텍스트 정제의 과정을 거친 후에는 다시 한번 빈(empty) 샘플이 생겼는지 확인해보는 것이 좋아요. 정제 전에는 데이터가 존재했지만, 정제 과정에서 문장의 모든 단어가 사라지는 경우가 있을 수 있어요. 이렇게 되면 샘플 자체가 빈 값을 가지게 되겠죠.\n",
    "\n",
    "보다 쉽게 확인하기 위해 데이터들을 데이터프레임에 재저장할게요. 빈(empty) 값을 가진 샘플들이 있다면, 모두 Null 값을 가진 샘플로 대체해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a223334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7d42383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary\n",
       "0  bought several vitality canned dog food produc...  good quality dog food\n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised\n",
       "2  confection around centuries light pillowy citr...    delight says it all\n",
       "3  looking secret ingredient robitussin believe f...         cough medicine\n",
       "4  great taffy great price wide assortment yummy ...            great taffy"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20f3ae",
   "metadata": {},
   "source": [
    "이전과 같이 .isnull().sum()을 사용해서 Null 값이 생겼는지 해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6aec2be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text        0\n",
       "Summary    70\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa769ba6",
   "metadata": {},
   "source": [
    "Summary 열에서 70개의 Null 값이 생겼네요. 원래는 단어가 있었는데, 정제 과정에서 모든 단어가 제거되어 빈 샘플이 70개나 생겼다는 의미예요. 이 샘플들은 모두 제거해줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "170313b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 88355\n"
     ]
    }
   ],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ebac7",
   "metadata": {},
   "source": [
    "# 3-6. 데이터 전처리하기 (2) 훈련데이터와 테스트데이터 나누기 | 30분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8d2978",
   "metadata": {},
   "source": [
    "## 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf2b62c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 2\n",
      "텍스트의 최대 길이 : 1235\n",
      "텍스트의 평균 길이 : 38.792428272310566\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 28\n",
      "요약의 평균 길이 : 4.010729443721352\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821b1d0",
   "metadata": {},
   "source": [
    "위의 그래프처럼, 많은 양의 데이터를 다룰 때는 데이터를 시각화하여 보는 것이 도움이 돼요. 위에서부터 차례대로 그래프는 각각 실제 텍스트와 요약의 길이 분포, 실제 텍스트 샘플 길이별 개수, 요약본 샘플 길이별 개수를 나타내고 있어요.\n",
    "\n",
    "Text의 경우 최소 길이가 2, 최대 길이가 1,235로 그 차이가 굉장히 크죠. 하지만 평균 길이는 38로 시각화된 그래프로 봤을 때는 대체적으로는 100 내외의 길이를 가진다는 것을 확인할 수 있어요.\n",
    "\n",
    "Summary의 경우 최소 길이가 1, 최대 길이가 28, 그리고 평균 길이가 4로 Text에 비해 상대적으로 길이가 매우 짧아요. 그래프로 봤을 때에도 대체적으로 10이하의 길이를 가지고 있네요.\n",
    "\n",
    "이로부터 Text의 최대 길이와 Summary의 적절한 최대 길이를 임의로 정해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a630b0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "text_max_len = 50\n",
    "summary_max_len = 8\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a975c",
   "metadata": {},
   "source": [
    "각각 50과 8로 정했는데 이 길이를 선택했을 때, 얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인하는 편이 객관적으로 길이를 결정하는 데 도움이 될거예요. 훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수를 만들어서 좀 더 정확하게 판단해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "018a220a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b42f83",
   "metadata": {},
   "source": [
    "이렇게 만든 함수를 Text와 Summary에 적용해 우리가 결정한 임의의 길이가 몇%의 샘플까지 포함하는지 볼 수 있겠죠."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "696c021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 50 이하인 샘플의 비율: 0.7745119121724859\n",
      "전체 샘플 중 길이가 8 이하인 샘플의 비율: 0.9424593967517402\n"
     ]
    }
   ],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551f7b59",
   "metadata": {},
   "source": [
    "각각 50과 8로 패딩을 하게 되면 해당 길이보다 긴 샘플들은 내용이 잘리게 되는데, Text 열의 경우에는 약 23%의 샘플들이 내용이 망가지게 된다고 하네요.\n",
    "\n",
    "우리는 정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제할게요.\n",
    "\n",
    "Q. Text와 Summary를 담고 있는 data 데이터프레임을 위에서 임의로 정의한 text_max_len과 summary_max_len의 길이보다 큰 샘플을 제외하는 코드를 작성하세요.\n",
    "(힌트 : apply 함수와 lamda식을 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "296d3ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "필터링 전 샘플 수: 88355\n",
      "필터링 후 샘플 수: 65818\n"
     ]
    }
   ],
   "source": [
    "# [[YOUR CODE]]\n",
    "# 조건에 맞는 샘플만 필터링\n",
    "filtered_data = data[\n",
    "    data['Text'].apply(lambda x: len(x.split()) <= text_max_len) &  # Text 길이 조건\n",
    "    data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)  # Summary 길이 조건\n",
    "]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"필터링 전 샘플 수: {len(data)}\")\n",
    "print(f\"필터링 후 샘플 수: {len(filtered_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c92fa2",
   "metadata": {},
   "source": [
    "## 시작 토큰과 종료 토큰 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6056b",
   "metadata": {},
   "source": [
    "seq2seq 훈련을 위해서는 디코더의 입력과 레이블에 시작 토큰과 종료 토큰을 추가할 필요가 있어요. 이번 실습에서는 시작 토큰은 sostoken, 종료 토큰은 eostoken이라 임의로 명명하고 앞, 뒤로 추가할 거예요. 디코더의 입력에 해당하면서 시작 토큰이 맨 앞에 있는 문장의 이름을 decoder_input, 디코더의 출력 또는 레이블에 해당되면서 종료 토큰이 맨 뒤에 붙는 문장의 이름을 decoder_target이라고 이름을 정했어요. 두 개의 문장 모두 Summary 열로부터 만들 거예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73238620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>good quality dog food</td>\n",
       "      <td>sostoken good quality dog food</td>\n",
       "      <td>good quality dog food eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>not as advertised</td>\n",
       "      <td>sostoken not as advertised</td>\n",
       "      <td>not as advertised eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>delight says it all</td>\n",
       "      <td>sostoken delight says it all</td>\n",
       "      <td>delight says it all eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>cough medicine</td>\n",
       "      <td>sostoken cough medicine</td>\n",
       "      <td>cough medicine eostoken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>great taffy</td>\n",
       "      <td>sostoken great taffy</td>\n",
       "      <td>great taffy eostoken</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                Summary  \\\n",
       "0  bought several vitality canned dog food produc...  good quality dog food   \n",
       "1  product arrived labeled jumbo salted peanuts p...      not as advertised   \n",
       "2  confection around centuries light pillowy citr...    delight says it all   \n",
       "3  looking secret ingredient robitussin believe f...         cough medicine   \n",
       "4  great taffy great price wide assortment yummy ...            great taffy   \n",
       "\n",
       "                    decoder_input                  decoder_target  \n",
       "0  sostoken good quality dog food  good quality dog food eostoken  \n",
       "1      sostoken not as advertised      not as advertised eostoken  \n",
       "2    sostoken delight says it all    delight says it all eostoken  \n",
       "3         sostoken cough medicine         cough medicine eostoken  \n",
       "4            sostoken great taffy            great taffy eostoken  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d0b58",
   "metadata": {},
   "source": [
    "앞뒤로 토큰이 잘 붙었죠? 인코더의 입력, 디코더의 입력과 레이블을 각각 다시 Numpy 타입으로 저장해 줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "228b7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a128ae1b",
   "metadata": {},
   "source": [
    "이제 훈련 데이터와 테스트 데이터를 분리할거에요.\n",
    "\n",
    "훈련 데이터와 테스트 데이터를 분리하는 방법은 분리 패키지를 사용하는 방법, 또는 직접 코딩을 통해서 분리하는 방법 등 여러 가지 방법이 있을 텐데 여기서는 직접 해볼게요. 우선, encoder_input과 크기와 형태가 같은 순서가 섞인 정수 시퀀스를 만들어줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "039810e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53325 34092 65373 ... 37488  5801 10898]\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e1e8530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ec337fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 17671\n"
     ]
    }
   ],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "46941800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 개수 : 70684\n",
      "훈련 레이블의 개수 : 70684\n",
      "테스트 데이터의 개수 : 17671\n",
      "테스트 레이블의 개수 : 17671\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddf3f21",
   "metadata": {},
   "source": [
    "# 3-7. 데이터 전처리하기 (3) 정수 인코딩 | 20분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0716a3f",
   "metadata": {},
   "source": [
    "## 단어 집합(vocabulary) 만들기 및 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879ef0",
   "metadata": {},
   "source": [
    "이제 기계가 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꾸어 주어야 해요. 이를 위해서는 각 단어에 고유한 정수를 맵핑하는 작업이 필요해요. 이 과정을 단어 집합(vocabulary) 을 만든다고 표현해요. 훈련 데이터에 대해서 단어 집합을 만들어볼게요. 우선, 원문에 해당되는 encoder_input_train에 대해서 단어 집합을 만들게요.\n",
    "\n",
    "Keras의 토크나이저를 사용하면, 입력된 훈련 데이터로부터 단어 집합을 만들 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bce0ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "258e1819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('like', 1), ('good', 2), ('taste', 3), ('one', 4), ('would', 5)]\n"
     ]
    }
   ],
   "source": [
    "# ERROR\n",
    "# print(src_tokenizer[:5])\n",
    "\n",
    "print(list(src_tokenizer.word_index.items())[:5])  # 단어-인덱스 맵핑 상위 5개 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb6f06b",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었어요. 현재 생성된 단어 집합은 src_tokenizer.word_index에 저장되어 있어요. 그런데 우리는 이렇게 만든 단어 집합에 있는 모든 단어를 사용하는 것이 아니라, 빈도수가 낮은 단어들은 훈련 데이터에서 제외하고 진행하려고 해요.\n",
    "\n",
    "등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼게요.\n",
    "\n",
    "src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장돼 있는데, 이를 통해서 통계적인 정보를 얻을 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d146a3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 49077\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 35853\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 13224\n",
      "단어 집합에서 희귀 단어의 비율: 73.05458768873403\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 2.3652864311809187\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1f05b8",
   "metadata": {},
   "source": [
    "encoder_input_train에는 3만여 개의 단어가 있네요. 그 아래의 통계 정보들을 해석해볼까요?\n",
    "\n",
    "등장 빈도가 threshold 값인 7회 미만, 즉 6회 이하인 단어들은 단어 집합에서 무려 70% 이상을 차지하네요. 하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 적은 수치인 3.39%밖에 되지 않아요.\n",
    "\n",
    "그래서 등장 빈도가 6회 이하인 단어들은 정수 인코딩 과정에서 빼고, 훈련 데이터에서 제거하고자 합니다. 위에서 이를 제외한 단어 집합의 크기를 8천여 개로 계산했는데, 이와 비슷한 값으로 어림잡아 단어 집합의 크기를 8,000으로 제한해볼게요. 토크나이저를 정의할 때 num_words의 값을 정해주면, 단어 집합의 크기를 제한할 수 있어요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80c5c2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710684e",
   "metadata": {},
   "source": [
    "texts_to_sequences()는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행해요. 현재 단어 집합의 크기를 8,000으로 제한했으니까 이제 8,000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않아요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3312f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1140, 3336, 832, 129, 283, 284, 54, 527, 859, 485, 2184, 158, 660, 327, 455, 152, 660, 2855, 3845, 340, 902, 4190, 6497, 384, 118, 1189, 2170, 3275, 109, 491, 663, 2822, 170, 478, 6240, 1447, 95, 894, 759, 476, 385, 6023, 1230, 3195, 6240, 1864, 1606, 3336, 360, 293, 4191, 6024, 2184, 153, 31, 3808, 550, 156, 973, 684, 3275, 389, 418, 153, 2567, 3195, 6240, 1165, 1297, 443, 25, 6632, 2184, 20, 161, 4, 152, 286, 4439, 551, 48, 26, 105, 1306, 109, 491, 360, 748, 96, 408], [1051, 1428, 34, 7, 2748, 1027, 263, 77, 7471, 3226, 5933, 59, 32, 7902, 3305, 3, 1, 263, 1948, 53, 1, 786, 1543, 1741], [262, 3913, 4678, 587, 618, 4632, 506, 372, 2260, 2102, 5841, 93, 3913, 4678, 607, 618, 598, 598, 2537, 2628, 5171, 3306, 6771, 105, 598, 103, 482, 736, 2618, 1837, 4033, 3535, 598, 5390, 436, 3436, 4678, 607, 915, 3461, 479, 1451, 12, 541, 553, 1, 2628, 5171, 1235, 4385, 608, 3048, 179, 105, 92, 607, 1231, 598, 91, 479, 3992, 1379, 1077, 1640, 105, 261, 7280, 290, 3337, 463, 6498, 1690, 5171, 4582, 7087, 1077, 3683, 2231, 457, 705, 12, 2118, 348, 2211, 541, 2125, 7687, 7472]]\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b983f5",
   "metadata": {},
   "source": [
    "Summary 데이터에 대해서도 동일한 작업을 수행할게요. 케라스의 토크나이저를 사용하여 decoder_input_train을 입력으로 전체 단어 집합과 각 단어에 대한 빈도수를 계산해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22db15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d0b224",
   "metadata": {},
   "source": [
    "이제 단어 집합이 생성되는 동시에 각 단어에 고유한 정수가 부여되었어요. 이는 tar_tokenizer.word_index에 저장되어 있어요. tar_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장돼 있는데, 이를 통해서 통계적인 정보를 얻어서, 등장 빈도수가 6회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f62c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 13202\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 9986\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 3216\n",
      "단어 집합에서 희귀 단어의 비율: 75.64005453719133\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.766539160263137\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a31f7",
   "metadata": {},
   "source": [
    "등장 빈도가 5회 이하인 단어들은 단어 집합에서 약 77%를 차지하고 있네요. 하지만 실제로 훈련 데이터에서 등장 빈도로 차지하는 비중은 상대적으로 매우 적은 수치인 5.87%밖에 되지 않아요. 아까 했던 것과 동일하게 이 단어들은 모두 제거할게요. 어림잡아 2,000을 단어 집합의 크기로 제한할게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7444f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 341, 48, 67, 42, 11], [1, 3, 139, 17, 151], [1, 516, 29], [1, 11, 24, 44, 9, 39, 62, 4, 99], [1, 10, 40]]\n",
      "target\n",
      "decoder  [[341, 48, 67, 42, 11, 2], [3, 139, 17, 151, 2], [516, 29, 2], [11, 24, 44, 9, 39, 62, 4, 99, 2], [10, 40, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f787fa0",
   "metadata": {},
   "source": [
    "정상적으로 정수 인코딩 작업이 끝났어요. 현재 decoder_input_train과 decoder_target_train에는 더 이상 숫자 2,000이 넘는 숫자들은 존재하지 않아요. 그런데 다음 작업인 패딩 하기로 넘어가기 전에 한 가지 점검해야 할 것이 있어요.\n",
    "\n",
    "전체 데이터에서 빈도수가 낮은 단어가 삭제되었다는 것은 빈도수가 낮은 단어만으로 구성되었던 샘플들은 이제 빈(empty) 샘플이 되었을 가능성이 있어요. 이 현상은 길이가 상대적으로 길었던 원문(Text)의 경우에는 문제가 별로 없겠지만, 애초에 평균 길이가 4밖에 되지 않았던 요약문(Summary)의 경우에는 이 현상이 굉장히 두드러졌을 가능성이 높겠죠.\n",
    "\n",
    "요약문에서 길이가 0이 된 샘플들의 인덱스를 받아와볼게요. 여기서 주의할 점은 요약문인 decoder_input에는 sostoken 또는 decoder_target에는 eostoken이 추가된 상태이고, 이 두 토큰은 모든 샘플에서 등장하므로 빈도수가 샘플 수와 동일하게 매우 높으므로 단어 집합 제한에도 삭제되지 않아요. 그래서 이제 길이가 0이 된 요약문의 실제 길이는 1로 나올 거예요. 길이 0이 된 decoder_input에는 sostoken, decoder_target에는 eostoken만 남아 있을 테니까요.\n",
    "\n",
    "훈련 데이터와 테스트 데이터에 대해서 요약문의 길이가 1인 경우의 인덱스를 각각 drop_train과 drop_test에 라는 변수에 저장해볼게요. 이 샘플들은 모두 삭제할 거예요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a83ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 1561\n",
      "삭제할 테스트 데이터의 개수 : 442\n",
      "훈련 데이터의 개수 : 69123\n",
      "훈련 레이블의 개수 : 69123\n",
      "테스트 데이터의 개수 : 17229\n",
      "테스트 레이블의 개수 : 17229\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f5a69",
   "metadata": {},
   "source": [
    "## 패딩하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efca511",
   "metadata": {},
   "source": [
    "텍스트 시퀀스를 정수 시퀀스로 변환했다면, 이제 서로 다른 길이의 샘플들을 병렬 처리하기 위해 같은 길이로 맞춰주는 패딩 작업을 해주어야 해야 해요. 아까 정해두었던 최대 길이로 패딩 해 줄 거에요. 최대 길이보다 짧은 데이터들은 뒤의 공간에 숫자 0을 넣어 최대 길이로 길이를 맞춰줄게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bcc3d7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5bf7446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_train:  [[ 385 6023 1230 ...  748   96  408]\n",
      " [1051 1428   34 ...    0    0    0]\n",
      " [ 915 3461  479 ... 2125 7687 7472]\n",
      " ...\n",
      " [ 156  130  396 ...   31   22 1514]\n",
      " [ 280  480  140 ...    0    0    0]\n",
      " [ 105    5   21 ...    0    0    0]] \n",
      " 69123 개 ( 50 )\n",
      "encoder_input_test:  [[  10   69   74 ...    0    0    0]\n",
      " [2239  334  792 ...  715  281  349]\n",
      " [  11  442 2155 ...    0    0    0]\n",
      " ...\n",
      " [ 385 1145  595 ...  176  315  434]\n",
      " [ 290 1232   69 ...  695  176  315]\n",
      " [ 417  403  255 ...    0    0    0]] \n",
      " 17229 개 ( 50 )\n",
      "decoder_input_train:  [[  1 341  48 ...  11   0   0]\n",
      " [  1   3 139 ...   0   0   0]\n",
      " [  1 516  29 ...   0   0   0]\n",
      " ...\n",
      " [ 12  68 511 ... 383  11  43]\n",
      " [  1  92   3 ...   0   0   0]\n",
      " [  1   6  11 ...   5   7  32]] \n",
      " 69123 개 ( 8 )\n",
      "decoder_target_train:  [[341  48  67 ...   2   0   0]\n",
      " [  3 139  17 ...   0   0   0]\n",
      " [516  29   2 ...   0   0   0]\n",
      " ...\n",
      " [ 68 511   5 ...  11  43   2]\n",
      " [ 92   3  12 ...   0   0   0]\n",
      " [  6  11  38 ...   7  32   2]] \n",
      " 69123 개 ( 8 )\n",
      "decoder_input_test:  [[   1  210   73 ...    0    0    0]\n",
      " [   1  284    5 ...    0    0    0]\n",
      " [   1    3   27 ...   55 1878    0]\n",
      " ...\n",
      " [   1   17  337 ...    0    0    0]\n",
      " [   1    5  296 ...   66    5  278]\n",
      " [   1    3  609 ...   25    8  206]] \n",
      " 17229 개 ( 8 )\n",
      "decoder_target_test:  [[ 210   73    2 ...    0    0    0]\n",
      " [ 284    5  378 ...    0    0    0]\n",
      " [   3   27    8 ... 1878    2    0]\n",
      " ...\n",
      " [  17  337    2 ...    0    0    0]\n",
      " [   5  296 1056 ...    5  278    2]\n",
      " [   3  609    7 ...    8  206    2]] \n",
      " 17229 개 ( 8 )\n"
     ]
    }
   ],
   "source": [
    "print(\"encoder_input_train: \", encoder_input_train, \"\\n\", len(encoder_input_train), \"개 (\", len(encoder_input_train[0]), \")\")\n",
    "print(\"encoder_input_test: \", encoder_input_test, \"\\n\", len(encoder_input_test), \"개 (\", len(encoder_input_test[0]), \")\")\n",
    "print(\"decoder_input_train: \", decoder_input_train, \"\\n\", len(decoder_input_train), \"개 (\", len(decoder_input_train[0]), \")\")\n",
    "print(\"decoder_target_train: \", decoder_target_train, \"\\n\", len(decoder_target_train), \"개 (\", len(decoder_target_train[0]), \")\")\n",
    "print(\"decoder_input_test: \", decoder_input_test, \"\\n\", len(decoder_input_test), \"개 (\", len(decoder_input_test[0]), \")\")\n",
    "print(\"decoder_target_test: \", decoder_target_test, \"\\n\", len(decoder_target_test), \"개 (\", len(decoder_target_test[0]), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78851a",
   "metadata": {},
   "source": [
    "# 3-8. 모델 설계하기 | 30분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e31f460",
   "metadata": {},
   "source": [
    "이제는 모델을 설계할 시간이에요. 우선 함수형 API를 이용해서 인코더를 설계해 볼게요.\n",
    "\n",
    "Q.인코더 LSTM 1을 참고해서 나머지 인코더의 LSTM 2, LSTM 3의 코드를 완성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "94e78028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "# encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "# [[YOUR CODE]]\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "# [[YOUR CODE]]\n",
    "# \n",
    "# encoder_lstm3 = LSTM(hidden_size, return_sequences=False, return_state=True, dropout=0.4)\n",
    "# \n",
    "# GPT-4o의 트롤짓... 뜬금없이 여기서 return_sequences=False를 줘버려서 밑에서 ValueError 터졌었음.\n",
    "# ValueError: Input 3 is incompatible with layer model_7: expected shape=(None, 50, 256), found shape=(None, 256)\n",
    "\n",
    "\n",
    "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output3, state_h3, state_c3 = encoder_lstm3(encoder_output2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb127410",
   "metadata": {},
   "source": [
    "임베딩 벡터의 차원은 128로 정의하고, hidden state의 크기를 256으로 정의했어요. hidden state는 LSTM에서 얼만큼의 수용력(capacity)를 가질지를 정하는 파라미터에요. 이 파라미터는 LSTM의 용량의 크기나, LSTM에서의 뉴런의 개수라고 이해하면 돼요. 다른 신경망과 마찬가지로, 무조건 용량을 많이 준다고 해서 성능이 반드시 올라가는 것은 아니에요.\n",
    "\n",
    "인코더의 LSTM은 총 3개의 층으로 구성해서 모델의 복잡도를 높였어요. hidden state의 크기를 늘리는 것이 LSTM 층 1개의 용량을 늘린다면, 3개의 층을 사용하는 것은 모델의 용량을 늘린다고 볼 수 있죠. 3개의 층을 지나서 인코더로부터 나온 출력 벡터는 디코더로 보내줘야겠죠?\n",
    "\n",
    "또한 LSTM은 dropout 뿐 아니라 recurrent dropout까지 사용할 수 있어요. 일반적인 dropout은 레이어의 weight를 랜덤으로 생략하여 모델의 과적합(overfitting)을 해결해주는 방법이에요.\n",
    "\n",
    "반면 recurrent dropout은 dropout을 레이어가 아닌 time step마다 해주는 방식이에요. 즉 time step의 입력을 랜덤으로 생략해 주는 거죠. recurrent dropout은 일반적인 dropout와 같이 regularization을 해주는 효과가 있고, 과적합을 방지할 수 있다고 해요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382062c0",
   "metadata": {},
   "source": [
    "참고로 recurrent dropout을 사용하면 아래와 같은 경고문이 뜹니다.\n",
    "\n",
    "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
    "recurrent dropout을 사용할 시 cuDNN을 사용할 수 없어서 recurrent dropout을 사용하지 않을 때보다 학습 시간이 오래 걸립니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "23b7fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My Code\n",
    "encoder_outputs, state_h, state_c = encoder_output3, state_h3, state_c3\n",
    "\n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "\n",
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "# decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c66c42",
   "metadata": {},
   "source": [
    "디코더의 임베딩 층과 LSTM을 설계하는 것은 인코더와 거의 동일해요. 하지만 LSTM의 입력을 정의할 때, initial_state의 인자값으로 인코더의 hidden state와 cell state의 값을 넣어줘야 해요.\n",
    "\n",
    "디코더의 출력층을 설계해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9d0ba523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 50, 128)      1024000     input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, 50, 256), (N 394240      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  [(None, 50, 256), (N 525312      lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 128)    256000      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  [(None, 50, 256), (N 525312      lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, None, 256),  394240      embedding_10[0][0]               \n",
      "                                                                 lstm_18[0][1]                    \n",
      "                                                                 lstm_18[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 2000)   514000      lstm_20[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,633,104\n",
      "Trainable params: 3,633,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480bcf8",
   "metadata": {},
   "source": [
    "디코더의 출력층에서는 Summary의 단어장인 tar_vocab의 수많은 선택지 중 하나의 단어를 선택하는 다중 클래스 분류 문제를 풀어야 해요. 그렇기 때문에 Dense의 인자로 tar_vocab을 주고, 활성화 함수로 소프트맥스 함수를 사용하고 있어요.\n",
    "\n",
    "지금까지 설계한 것은 인코더의 hidden state와 cell state를 디코더의 초기 state로 사용하는 가장 기본적인 seq2seq에요. 그런데 디코더의 출력층을 설계를 살짝 바꿔서 성능을 높일 수 있는 방법이 있어요! 바로 어텐션 메커니즘이에요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1270e35",
   "metadata": {},
   "source": [
    "## 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b5a7d5",
   "metadata": {},
   "source": [
    "어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야 한다는 뜻이에요. 어텐션 함수를 설계해보는 것은 다음 기회로 미루기로 하고, 여기서는 TensorFlow에 이미 구현된 어텐션 함수를 가져와서 디코더의 출력층에 어떤 방식으로 결합하는지 배워볼게요. 참고로 여기서 사용하는 어텐션 함수는 Bahdanau 스타일의 어텐션입니다. 이 어텐션에 대한 자세한 설명은 텐서플로우 홈페이지를 참고하세요.\n",
    "\n",
    "아래와 같이 어텐션 층을 만들고, 위에서 설계한 디코더의 출력층을 수정해 봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "94dffc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_22 (InputLayer)           [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 50, 128)      1024000     input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  [(None, 50, 256), (N 394240      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_17 (LSTM)                  [(None, 50, 256), (N 525312      lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 128)    256000      input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_18 (LSTM)                  [(None, 50, 256), (N 525312      lstm_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, None, 256),  394240      embedding_10[0][0]               \n",
      "                                                                 lstm_18[0][1]                    \n",
      "                                                                 lstm_18[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_20[0][0]                    \n",
      "                                                                 lstm_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_20[0][0]                    \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 2000)   1026000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,145,360\n",
      "Trainable params: 4,145,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1b763",
   "metadata": {},
   "source": [
    "위의 코드는 인코더의 hidden state들과 디코더의 hidden state들을 어텐션 함수의 입력으로 사용하고, 어텐션 함수가 리턴한 값을 예측 시에 디코더의 hidden state와 함께 활용하는 형태로 작동하고 있어요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8643c2",
   "metadata": {},
   "source": [
    "# 3-9. 모델 훈련하기 | 30분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "780ab1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "271/271 [==============================] - 24s 71ms/step - loss: 2.9612 - val_loss: 2.7362\n",
      "Epoch 2/50\n",
      "271/271 [==============================] - 18s 67ms/step - loss: 2.6289 - val_loss: 2.5888\n",
      "Epoch 3/50\n",
      "271/271 [==============================] - 18s 68ms/step - loss: 2.4272 - val_loss: 2.4198\n",
      "Epoch 4/50\n",
      "271/271 [==============================] - 19s 69ms/step - loss: 2.3004 - val_loss: 2.3159\n",
      "Epoch 5/50\n",
      "271/271 [==============================] - 19s 69ms/step - loss: 2.2136 - val_loss: 2.2494\n",
      "Epoch 6/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 2.1481 - val_loss: 2.1805\n",
      "Epoch 7/50\n",
      "271/271 [==============================] - 18s 68ms/step - loss: 2.0941 - val_loss: 2.1539\n",
      "Epoch 8/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 2.0494 - val_loss: 2.1430\n",
      "Epoch 9/50\n",
      "271/271 [==============================] - 19s 69ms/step - loss: 2.0088 - val_loss: 2.1170\n",
      "Epoch 10/50\n",
      "271/271 [==============================] - 19s 69ms/step - loss: 1.9733 - val_loss: 2.0944\n",
      "Epoch 11/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 1.9392 - val_loss: 2.0859\n",
      "Epoch 12/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 1.9092 - val_loss: 2.0790\n",
      "Epoch 13/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 1.8797 - val_loss: 2.0781\n",
      "Epoch 14/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 1.8530 - val_loss: 2.0696\n",
      "Epoch 15/50\n",
      "271/271 [==============================] - 18s 68ms/step - loss: 1.8260 - val_loss: 2.0850\n",
      "Epoch 16/50\n",
      "271/271 [==============================] - 19s 69ms/step - loss: 1.8016 - val_loss: 2.0640\n",
      "Epoch 17/50\n",
      "271/271 [==============================] - 19s 68ms/step - loss: 1.7767 - val_loss: 2.0676\n",
      "Epoch 18/50\n",
      "271/271 [==============================] - 19s 69ms/step - loss: 1.7542 - val_loss: 2.0684\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955f879",
   "metadata": {},
   "source": [
    "'조기 종료'를 뜻하는 EarlyStopping은 특정 조건이 충족되면 훈련을 멈추는 역할을 해요.\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "위 코드에서는 val_loss(검증 데이터의 손실)을 관찰하다가, 검증 데이터의 손실이 줄어들지 않고 증가하는 현상이 2회(patience=2) 관측되면 학습을 멈추도록 설정돼 있어요. EarlyStopping이 작동한다면 epochs가 아무리 크게 설정되어 있어도 모델 훈련을 최적점에서 멈출 수 있겠네요.\n",
    "\n",
    "EarlyStopping에 대한 자세한 내용은 아래 링크를 참고하세요! 자주 쓰이는 도구이니 자세히 알아두면 매우 도움이 될 거예요!!\n",
    "\n",
    "Early Stopping\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc66541",
   "metadata": {},
   "source": [
    "Q. Early Stopping을 사용할 경우 조심해야 하는 경우가 있는데, 어떤 경우일까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b98b44c",
   "metadata": {},
   "source": [
    "patience가 0이 아닌 경우에는 훈련이 종료되었을 때 성능이 최고인 상황이 아닐 수 있습니다. 예를들어 patience가 3인 경우, 15 epoch에서 loss가 감소하다가 16 epoch부터 loss가 증가한다면 18 epoch 때 모델을 저장하고 학습을 종료합니다. 그래서 학습 중에 모델을 저장하는 callback 함수를 같이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff5769",
   "metadata": {},
   "source": [
    "이제 훈련 데이터의 손실과 검증 데이터의 손실이 줄어드는 과정을 시각화 해봐요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77920d79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28b98c",
   "metadata": {},
   "source": [
    "# 3-10. 인퍼런스 모델 구현하기 | 30분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6f6ed085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6439c737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3e2a8e60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델 # OLD\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c, decoder_hidden_state_input],\n",
    "#     [decoder_outputs2] + [state_h2, state_c2])\n",
    "\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9819c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "#         output_tokens, h, c = decoder_model.predict([target_seq] + [e_h, e_c, e_out])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (summary_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d7059",
   "metadata": {},
   "source": [
    "# 3-11. 모델 테스트하기 | 15분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "10a4cbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    # [[YOUR CODE]]\n",
    "    temp = ''\n",
    "    for i in input_seq:\n",
    "        if i != 0 and i != tar_word_to_index['sostoken'] and i != tar_word_to_index['eostoken']:\n",
    "            temp += tar_index_to_word[i] + ' '  # 요약 단어 집합에서 정수 -> 단어 변환\n",
    "    return temp\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ac0673b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : year searching trying new recipes bread mix use egg free version substituting applesauce mixed egg make two mini loaves order bread fall cooking time needs longer applesauce also makes great rolls bread bread like texture whole family likes also holds well week without freezing \n",
      "실제 요약 : fabulous allergy free bread \n",
      "예측 요약 :  added quality original might weak else bargain\n",
      "\n",
      "\n",
      "원문 : expect anything fancy cake mix buy taste alone rises cake mixes purchased naturally flavor subtle sublime perfect chocolate frosting good enough everyday \n",
      "실제 요약 : good chocolate cake \n",
      "예측 요약 :  added quality original weak weak oil perfect\n",
      "\n",
      "\n",
      "원문 : quite strength british tea pg tips makes superb iced tea \n",
      "실제 요약 : superb tea \n",
      "예측 요약 :  lowrey yet pro candies remember deceiving latte\n",
      "\n",
      "\n",
      "원문 : wild rice blend single bags hard find area stores buy six bags time leads problems ie long term storage mailed producer asking best method twice yet receive reply would nice would bag containers paul net \n",
      "실제 요약 : great product but \n",
      "예측 요약 :  added quality original weak might else bargain\n",
      "\n",
      "\n",
      "원문 : impressed flavors syrups worth premium price tag sure \n",
      "실제 요약 : mediocre at best \n",
      "예측 요약 :  lowrey yet yet heat solution latte popchips\n",
      "\n",
      "\n",
      "원문 : us buying diet soft drinks sweetener used often becomes vital piece information even bother offering diet tonic sale tell us many diet still made want know order \n",
      "실제 요약 : where are the ingredients \n",
      "예측 요약 :  added breakfast lol quality original popular with\n",
      "\n",
      "\n",
      "원문 : good variety chocolate bad everyone says flavors great gives family friends variety latte steamer favorite hazelnut stick time good buy \n",
      "실제 요약 : like this but \n",
      "예측 요약 :  added quality lowrey punch cans dry seasonings\n",
      "\n",
      "\n",
      "원문 : terrible taste strong salt flavor purchased black pepper taste salt cheap beef jerky even sticker top plastic says cheap gas station jerky would last resort desperate snack \n",
      "실제 요약 : cheap gas jerky \n",
      "예측 요약 :  added breakfast cuppa control control mistake popular\n",
      "\n",
      "\n",
      "원문 : diced tomatoes green topped favorite seasonings crumbled publix saltines result much improved meal significantly improved taste actually cannot see reason buying product would better buy orzo chicken favorite type cheese likely seasonings already want add buy fresh steam minutes throw mix add whatever imagination taste buds find far superior meal \n",
      "실제 요약 : needs help \n",
      "예측 요약 :  control im cereal count snack cans superior\n",
      "\n",
      "\n",
      "원문 : weight gain also camping like take batch tuna cups along cases cup size right one serving minor problem latest tuna cups might batch problem latest tuna dry almost choke seemed rather tasteless cups past seemed much better satisfied present ones however like product serving size deter future purchases gary peterson \n",
      "실제 요약 : good nutrition and right size \n",
      "예측 요약 :  lol quality original perfect popular with tully\n",
      "\n",
      "\n",
      "원문 : children age work always go product line helped us immensely mornings afternoon snacks well easy way kids grab healthy snack homework rush door afternoon practices also easy snack toss worry eating school going trade away favorite snack super banana particular favorite often fight last one \n",
      "실제 요약 : great for busy \n",
      "예측 요약 :  paris added cracker supermarket added game healthier\n",
      "\n",
      "\n",
      "원문 : lovely pink drink packaging cute inviting already wanting enjoy drink actually quite good always best chilled mix strawberries unique combination slightly sour sweet like carbonation would strange juice like juice natural fruit flavor good without additional sugar corn syrup would recommend giving try \n",
      "실제 요약 : fruity refreshing \n",
      "예측 요약 :  added game bodied lowrey punch far bodied\n",
      "\n",
      "\n",
      "원문 : recommended suitable cats received appeared far large kids tried dogs absolutely loved realized easily broken smaller pieces tossed bits kitties dishes also unlike cat treats would say give try \n",
      "실제 요약 : not let the large size put you off \n",
      "예측 요약 :  added quality original weak cats brownies hershey\n",
      "\n",
      "\n",
      "원문 : holy cow never seen much grease boxed product like also enough salt kill entire family use garden eat \n",
      "실제 요약 : tasty but \n",
      "예측 요약 :  added breakfast quality lowrey pro carbonated cereal\n",
      "\n",
      "\n",
      "원문 : fantastic rice cooks perfectly rice cooker even pulled rice grains still hulls sprouted pot spare bedroom cannot thank enough gourmet house \n",
      "실제 요약 : great rice fine price even rice with it \n",
      "예측 요약 :  added breakfast lol quality original lowrey much\n",
      "\n",
      "\n",
      "원문 : love found accident went back get went least stores trying find finally find little bigger skittles gum inside lot gum course also come flavors inside bag love \n",
      "실제 요약 : love em \n",
      "예측 요약 :  added quality original weak weak oil perfect\n",
      "\n",
      "\n",
      "원문 : wife ordered cans sugar free must admit skeptical thought would give try boy amazed refreshing flavor importantly felt minutes drinking slept like baby placed another order today plan give friends trouble sleeping great concept customer life \n",
      "실제 요약 : not what expected \n",
      "예측 요약 :  added quality original might weak else bargain\n",
      "\n",
      "\n",
      "원문 : hopes licorice toffee end like getting discontinued keep coming come large bag plus pounds well worth paid dollars less half pound stuff liked occasionally seen individual gas station time time paid cents piece got least hundred plus pieces plenty share ashamed admit eaten bag ashamed say ordered bag great stuff \n",
      "실제 요약 : very much like \n",
      "예측 요약 :  gatorade added soups soups costco costco bean\n",
      "\n",
      "\n",
      "원문 : food keeps dog super healthy believe best happy bag ripped transit fact forced buy bigger bag smaller one significantly less money dog weighs lbs bag size takes months eat \n",
      "실제 요약 : broken bag \n",
      "예측 요약 :  added breakfast added baby cuppa brands brands\n",
      "\n",
      "\n",
      "원문 : husband loves theses like except sunflower seeds great snacking alot comes box \n",
      "실제 요약 : yummy \n",
      "예측 요약 :  lowrey yet yet heat solution solution delectable\n",
      "\n",
      "\n",
      "원문 : ordered thinking would yellow cake mix heavy cake brown said comes handy make coffee cake upside cake recipes basic mix recipes problem plain cake though disappointing get creative instead \n",
      "실제 요약 : gluten free basic mix \n",
      "예측 요약 :  quality original lowrey punch cans replacement punch\n",
      "\n",
      "\n",
      "원문 : family bag last hours well made sweet delicious aunt usually gets costco much lower price see amazon shop around buy worth \n",
      "실제 요약 : you will ever have but this is pricey \n",
      "예측 요약 :  added breakfast quality lowrey has may th\n",
      "\n",
      "\n",
      "원문 : golden retriever loves treats received amazon vine program bichon although little big carried around chewed anyone come near sure going able eat size protective retriever ate immediately came back far stomachs upset hopefully good digestive system another treat would buy \n",
      "실제 요약 : my dog says stars \n",
      "예측 요약 :  added quality original might else expensive dressing\n",
      "\n",
      "\n",
      "원문 : flavoring always eaten meat vegetable flavored food treats peanut butter must throwing may give treats neighbor see dog eat since reviews experiences give one star dog would think would eat anything give chance said buy another funbites flavor see like loves funbites count outer crunchy shell got nutty flavor scent \n",
      "실제 요약 : up his not peanut butter lover \n",
      "예측 요약 :  saffron chews easy easy angel feels treat\n",
      "\n",
      "\n",
      "원문 : good product cheapest price pound seen getting every time need \n",
      "실제 요약 : great for gluten free baking \n",
      "예측 요약 :  lowrey yet pro candies deceiving latte word\n",
      "\n",
      "\n",
      "원문 : conclusion bought espresso machine price point amazon best price could find think would buy one would definitely buy saeco much research people always pleased saeco would say pleased machine ability perform daily consistently offering rich espresso great crema next time would buy one allowed switch coffee perhaps one little attractive \n",
      "실제 요약 : great little machine except for the \n",
      "예측 요약 :  affordable added oz when nature nature supplement\n",
      "\n",
      "\n",
      "원문 : roasted peanuts grams sugar per gram serving grams saturated fat sodium nutritional values high including substantial levels niacin magnesium vitamin iron calcium course bit naturally occurring oil minimal need stirring like natural peanut butter daughters sometimes eat right spoon enjoy baking peanut butter cookies never use brand peanut butter try \n",
      "실제 요약 : lover who quality \n",
      "예측 요약 :  st seasonings off search paprika of stay\n",
      "\n",
      "\n",
      "원문 : best cocoa mix downside use much make cocoa goes quick \n",
      "실제 요약 : love this stuff \n",
      "예측 요약 :  lowrey yet pro pro easy easy refreshing\n",
      "\n",
      "\n",
      "원문 : works great cup brewer good tasting tea easy make quick cuppa \n",
      "실제 요약 : good taste and quality \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 요약 :  lowrey yet yet heat solution latte popchips\n",
      "\n",
      "\n",
      "원문 : tough ah cannot compare cap crunch heaven sake one natural preservatives sure going get bit tough open put bowl cereal microwave seconds let cool enjoy voila really good combined yogurt fruit seriously compare processed preservative laden stuff grocery stores give shot \n",
      "실제 요약 : do not to cereals \n",
      "예측 요약 :  added game healthier bodied lowrey punch chews\n",
      "\n",
      "\n",
      "원문 : overall good pudding spoiled use way much spice overpowers pudding downright putting \n",
      "실제 요약 : way too much \n",
      "예측 요약 :  lowrey yet pro yet guys fake pretzels\n",
      "\n",
      "\n",
      "원문 : strong love taste bit like black licorice makes gag looking strong coffee hoping would maybe extra flavored creamer learn enjoy \n",
      "실제 요약 : nasty after taste \n",
      "예측 요약 :  added breakfast quality lowrey pro carbonated cereal\n",
      "\n",
      "\n",
      "원문 : cannot really leave fair review product im sure worked paired mother milk tea see results could body \n",
      "실제 요약 : not too sure \n",
      "예측 요약 :  added quality lowrey much much move gone\n",
      "\n",
      "\n",
      "원문 : order office everyone seems love much taste testing chose jack links great taste variety value \n",
      "실제 요약 : great taste \n",
      "예측 요약 :  added breakfast lowrey pro bodied joe kit\n",
      "\n",
      "\n",
      "원문 : product really good nd order placed extremely hot provide bit heat variety dishes particular order came form bag zip closure rather glass jar received st order product different packaging \n",
      "실제 요약 : excellent salt for extra heat \n",
      "예측 요약 :  added breakfast cuppa control control thick potato\n",
      "\n",
      "\n",
      "원문 : healthier candy bar full nuts little chunks dark chocolate chocolate good flavor bitter get craving candy bar reach brief case purse eat one instead sweet set teeth edge like candy bar fresh chewy wonder reviewer might got old batch something wrapped individually great throw back pack purse car snack run \n",
      "실제 요약 : dark chocolate peanut and almond bars \n",
      "예측 요약 :  aroma yum bait opinion chew surprised surprised\n",
      "\n",
      "\n",
      "원문 : got product month ago try first time cat pleased got mistake meat liquid meat least liquid chicken cat seriously crying right leg begging food usually curls goes sleep awhile eating get another brand emergencies case food arrive time need serious talk amazon \n",
      "실제 요약 : so disappointed where is the meat \n",
      "예측 요약 :  added added game healthier kibble pretzels serious\n",
      "\n",
      "\n",
      "원문 : earl lady grey still good irish breakfast tea may make break robust delightful clearing morning fog hearty way good variety pack imho keep hand office panel board break kitchen folks enjoy want tea break wonderful flavors aromas like cannot help make people sit notice make sometimes dull work day enjoyable \n",
      "실제 요약 : flavour and taste \n",
      "예측 요약 :  added thrilled peach crisp gross tasting tasting\n",
      "\n",
      "\n",
      "원문 : looking forward eating bars breakfast bit one surprised sweetness would eat breakfast every morning know would eat breakfast good bit sweeter side first glutino product tried first cereal bar glutino tried would buy maybe get different flavor like strawberry blueberry bars taste lot like fig newtons overall good \n",
      "실제 요약 : bit sweet but still good \n",
      "예측 요약 :  st cereal seasonings off last last air\n",
      "\n",
      "\n",
      "원문 : time want review also shipper shipper deserves stars chocolate shipped days extremely high temperatures worried shape going arrive surprise side chocolates food storage bag inside small foam cooler together ice packs neatly inside clean white carton box opened cooler contain still cold could impressed effort value merchandise free shipping wow \n",
      "실제 요약 : excellent \n",
      "예측 요약 :  refreshing memories cereal bait altoids bulk shipped\n",
      "\n",
      "\n",
      "원문 : convenient well priced two problems shipment notice arrives late modify current month order sent earlier prices seem change without prior notice \n",
      "실제 요약 : cat food \n",
      "예측 요약 :  added quality lowrey punch cans pear gone\n",
      "\n",
      "\n",
      "원문 : agree reviewer brand love sure realize average graham cracker one made wheat could gotten stale pack encountered taste great specifically look brand favorite \n",
      "실제 요약 : graham cracker as it is made of wheat \n",
      "예측 요약 :  added breakfast quality lowrey pro carbonated cereal\n",
      "\n",
      "\n",
      "원문 : tastes terrible used flavored protein strong flavor really gross hard mask wasted \n",
      "실제 요약 : gross \n",
      "예측 요약 :  saeco saeco tully they pro break storage\n",
      "\n",
      "\n",
      "원문 : brought forth family room extended likes could recall eye powerful warned expensive single reason buying big dog going lot food usually buying pound bag based current price pound bag would well bag sorry although dog clearly chow price prohibitive bottom line taste special dog sure like plus expensive big dog \n",
      "실제 요약 : very popular chow but \n",
      "예측 요약 :  added game healthier bodied does does tasted\n",
      "\n",
      "\n",
      "원문 : absolutely loves inconvenience find piece varies greatly size pretty large cut pieces appropriate size practical training time gets one little piece works great since cut find expensive really go long way important thing list ingredients full junk fact junk beef liver like really try know giving good quality foods treats \n",
      "실제 요약 : really good treats for training \n",
      "예측 요약 :  refreshing virgin bacon tofu hours cal let\n",
      "\n",
      "\n",
      "원문 : best flavor ever addicted good far best could live need anymore say \n",
      "실제 요약 : amazing \n",
      "예측 요약 :  lowrey yet pro candies deceiving latte weird\n",
      "\n",
      "\n",
      "원문 : given pound kona gift loved finding coffee use tasted close well looking cost per pound using starbucks blend per lb buying around per lb brainer know years ago honestly tell difference kona blend priced little folgers hate put fear price affected folks close kona get cheaper \n",
      "실제 요약 : best coffee for price \n",
      "예측 요약 :  crack authentic her auto stopped angel zing\n",
      "\n",
      "\n",
      "원문 : probably toddler favorite pouch admit lot lemon juice mix makes pouch somewhat sour toddler loves lemons though exactly taste unsure whether baby likes sour buy bulk probably acquired taste \n",
      "실제 요약 : very sour but baby loves it \n",
      "예측 요약 :  added quality original weak angel india kitten\n",
      "\n",
      "\n",
      "원문 : everything need build decorate gingerbread house kit year old daughter lot fun putting together impressed base came house able keep house weeks lot fun want make building gingerbread house holiday tradition \n",
      "실제 요약 : fun holiday \n",
      "예측 요약 :  added quality original weak angel india kitten\n",
      "\n",
      "\n",
      "원문 : best chips every tasted healthy great flavor buy often eat bag day lunch yummy \n",
      "실제 요약 : pop chips \n",
      "예측 요약 :  added breakfast lowrey pro bodied joe bring\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 100):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d9b7f1",
   "metadata": {},
   "source": [
    "# 3-12. 추출적 요약 해보기 | 20분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6ba3f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4305ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "34ba17d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The screen is filled with green, cascading code which gives way to the title, The Matrix.\r\n",
      "\r\n",
      "A phone rings and text appears on the screen: \"Call trans opt: received. 2-19-98 13:24:18 REC: Log>\" As a conversation takes place between Trinity (Carrie-Anne Moss) and Cypher (Joe Pantoliano), two free humans, a table of random green numbers are being scanned and individual numbers selected, creating a series of digits not unlike an ordinary phone number, as if a code is being deciphered or a call is being traced.\r\n",
      "\r\n",
      "Trinity discusses some unknown person. Cypher taunts Trinity, suggesting she enjoys watching him. Trinity counters that \"Morpheus (Laurence Fishburne) says he may be 'the One',\" just as the sound of a number being selected alerts Trinity that someone may be tracing their call. She ends the call.\r\n",
      "\r\n",
      "Armed policemen move down a darkened, decrepit hallway in the Heart O' the City Hotel, their flashlight beam bouncing just ahead of them. They come to room 303, kick down the door and find a woman dressed in black, facing away from them. It's Trinity. She brings her hands up from the laptop she's working on at their command.\r\n",
      "\r\n",
      "Outside the hotel a car drives up and three agents appear in neatly pressed black suits. They are Agent Smith (Hugo Weaving), Agent Brown (Paul Goddard), and Agent Jones (Robert Taylor). Agent Smith and the presiding police lieutenant argue. Agent Smith admonishes the policeman that they were given specific orders to contact the agents first, for their\n"
     ]
    }
   ],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fdbf1272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2b425308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "['Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.', 'Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.']\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0b8ebad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Trinity takes Neo to Morpheus.\n",
      "Morpheus, Trinity, Neo, Apoc, Switch, Mouse and Cypher are jacked into the Matrix.\n",
      "Trinity brings the helicopter down to the floor that Morpheus is on and Neo opens fire on the three Agents.\n"
     ]
    }
   ],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
