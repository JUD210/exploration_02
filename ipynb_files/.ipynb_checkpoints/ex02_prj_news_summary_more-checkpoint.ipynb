{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeacdd1f",
   "metadata": {},
   "source": [
    "# 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f848d",
   "metadata": {},
   "source": [
    "- \\#@ : 단순 노트\n",
    "- \\#@KeyNote : 핵심 개념 노트\n",
    "- \\#@Todo : 당장 해야할 것들"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9936a",
   "metadata": {},
   "source": [
    "# 프로젝트: 뉴스기사 요약해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "362edad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5\n",
      "2.6.0\n",
      "1.3.3\n",
      "1.2.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import nltk\n",
    "import tensorflow\n",
    "import summa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "\n",
    "print(nltk.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(pd.__version__)\n",
    "print(version('summa'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d0355",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c183b734",
   "metadata": {},
   "source": [
    "이 데이터는 기사의 본문에 해당되는 text와 headlines 두 가지 열로 구성되어져 있습니다.\n",
    "\n",
    "추상적 요약을 하는 경우에는 text를 본문, headlines를 이미 요약된 데이터로 삼아서 모델을 학습할 수 있어요. 추출적 요약을 하는 경우에는 오직 text열만을 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "90ec5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"../data/news_summary_more.csv\")\n",
    "data = pd.read_csv('../data/news_summary_more.csv', encoding='iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d406a1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47603</th>\n",
       "      <td>ED asks MEA to revoke Nirav Modi's passport</td>\n",
       "      <td>The Enforcement Directorate on Thursday wrote ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90568</th>\n",
       "      <td>Higuain's brace seals Juventus' 2-0 CL win aga...</td>\n",
       "      <td>Juventus' Argentine forward Gonzalo Higuain sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61181</th>\n",
       "      <td>Pakistan must end illegal occupation of PoK: I...</td>\n",
       "      <td>India has asked Pakistan at the UN to end its ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7910</th>\n",
       "      <td>I've nose and mouth again: Man who shot himsel...</td>\n",
       "      <td>America's 26-year-old Cameron Underwood, who g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5410</th>\n",
       "      <td>New species of snake discovered inside another...</td>\n",
       "      <td>Scientists have discovered remains of a new sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71316</th>\n",
       "      <td>Climate change may wipe out 33% of parasites b...</td>\n",
       "      <td>Changing climate could cause extinction of up ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73468</th>\n",
       "      <td>Two-time Olympic silver medalist arrested for ...</td>\n",
       "      <td>Australia's silver-winning Olympic medalist Ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81144</th>\n",
       "      <td>Actress killed by drug dealers over Ã¢ÂÂ¹6k p...</td>\n",
       "      <td>According to Mumbai Police, actress Kritika Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95138</th>\n",
       "      <td>Rani Mukerji starts shooting for her comeback ...</td>\n",
       "      <td>Actress Rani Mukerji has started shooting for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52571</th>\n",
       "      <td>Upcoming app to use blockchain tech to verify ...</td>\n",
       "      <td>Netherlands-based company LegalThings  is deve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "47603        ED asks MEA to revoke Nirav Modi's passport   \n",
       "90568  Higuain's brace seals Juventus' 2-0 CL win aga...   \n",
       "61181  Pakistan must end illegal occupation of PoK: I...   \n",
       "7910   I've nose and mouth again: Man who shot himsel...   \n",
       "5410   New species of snake discovered inside another...   \n",
       "71316  Climate change may wipe out 33% of parasites b...   \n",
       "73468  Two-time Olympic silver medalist arrested for ...   \n",
       "81144  Actress killed by drug dealers over Ã¢ÂÂ¹6k p...   \n",
       "95138  Rani Mukerji starts shooting for her comeback ...   \n",
       "52571  Upcoming app to use blockchain tech to verify ...   \n",
       "\n",
       "                                                    text  \n",
       "47603  The Enforcement Directorate on Thursday wrote ...  \n",
       "90568  Juventus' Argentine forward Gonzalo Higuain sc...  \n",
       "61181  India has asked Pakistan at the UN to end its ...  \n",
       "7910   America's 26-year-old Cameron Underwood, who g...  \n",
       "5410   Scientists have discovered remains of a new sp...  \n",
       "71316  Changing climate could cause extinction of up ...  \n",
       "73468  Australia's silver-winning Olympic medalist Ja...  \n",
       "81144  According to Mumbai Police, actress Kritika Ch...  \n",
       "95138  Actress Rani Mukerji has started shooting for ...  \n",
       "52571  Netherlands-based company LegalThings  is deve...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9ced7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@KeyNote 기존 코드 활용을 쉽게 하기 위해서 이름 변경.\n",
    "\n",
    "# 컬럼 이름 변경\n",
    "data.rename(columns={'headlines': 'Summary', 'text': 'Text'}, inplace=True)\n",
    "\n",
    "# 컬럼 순서 변경\n",
    "data = data[['Text', 'Summary']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "feae8b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63917</th>\n",
       "      <td>Students at United States' Southern Illinois U...</td>\n",
       "      <td>Poop repeatedly found in washing machines of U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61927</th>\n",
       "      <td>In a document submitted to the court, the CBI ...</td>\n",
       "      <td>Class XI Ryan student confessed to murder of P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87279</th>\n",
       "      <td>The government will be providing cheaper headp...</td>\n",
       "      <td>Tejas to get cheaper headphones after passenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89967</th>\n",
       "      <td>India-born brothers Srichand and Gopichand Hin...</td>\n",
       "      <td>India-born Hinduja brothers top UK's rich list...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8008</th>\n",
       "      <td>A 19-year-old British student, who clicked a p...</td>\n",
       "      <td>UK teen takes photo from plane, faces spying t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34865</th>\n",
       "      <td>A man has been arrested for allegedly raping a...</td>\n",
       "      <td>Man rapes 6-yr-old who stepped out of home to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15860</th>\n",
       "      <td>Mercedes-Benz parent Daimler has named Swedish...</td>\n",
       "      <td>Mercedes-Benz maker names first ever non-Germa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63786</th>\n",
       "      <td>Fourteen-time major winner American golfer Tig...</td>\n",
       "      <td>Tiger Woods' brand value dips Ã¢ÂÂ¹425 cr sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94785</th>\n",
       "      <td>Actor Aamir Khan has decided not to release 'D...</td>\n",
       "      <td>Aamir refuses Dangal release in Pak without Na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26347</th>\n",
       "      <td>A mid-air collision between two IndiGo planes ...</td>\n",
       "      <td>Mid-air collision averted between two IndiGo p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Text  \\\n",
       "63917  Students at United States' Southern Illinois U...   \n",
       "61927  In a document submitted to the court, the CBI ...   \n",
       "87279  The government will be providing cheaper headp...   \n",
       "89967  India-born brothers Srichand and Gopichand Hin...   \n",
       "8008   A 19-year-old British student, who clicked a p...   \n",
       "34865  A man has been arrested for allegedly raping a...   \n",
       "15860  Mercedes-Benz parent Daimler has named Swedish...   \n",
       "63786  Fourteen-time major winner American golfer Tig...   \n",
       "94785  Actor Aamir Khan has decided not to release 'D...   \n",
       "26347  A mid-air collision between two IndiGo planes ...   \n",
       "\n",
       "                                                 Summary  \n",
       "63917  Poop repeatedly found in washing machines of U...  \n",
       "61927  Class XI Ryan student confessed to murder of P...  \n",
       "87279  Tejas to get cheaper headphones after passenge...  \n",
       "89967  India-born Hinduja brothers top UK's rich list...  \n",
       "8008   UK teen takes photo from plane, faces spying t...  \n",
       "34865  Man rapes 6-yr-old who stepped out of home to ...  \n",
       "15860  Mercedes-Benz maker names first ever non-Germa...  \n",
       "63786  Tiger Woods' brand value dips Ã¢ÂÂ¹425 cr sin...  \n",
       "94785  Aamir refuses Dangal release in Pak without Na...  \n",
       "26347  Mid-air collision averted between two IndiGo p...  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88c99111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Text]         NASA has shared the final image taken by the Cassini spacecraft before it plunged into Saturn's atmosphere, ending its 20-year mission. Built in the early 1990s, the $3.9-billion spacecraft had a 1-megapixel camera, with which it took over 4,50,000 images of Saturn's rings and moons. The image here shows the site of atmospheric entry, taken at 6,34,000 kilometres from Saturn.\n",
      " -> 378문자\n",
      "\n",
      "[Summary]    NASA shares last image by spaceship that crashed into Saturn\n",
      " -> 60문자\n",
      "\n",
      "\n",
      "\n",
      "[Text]         A 25-year-old Hindu woman has moved the Kerala High Court against a Muslim man, alleging that he wanted to take her to Syria and sell her to ISIS as a sex slave. She claimed that he had forcibly converted her to Islam and forged documents to marry her. The woman also accused him of coercing her to have sexual intercourse. \n",
      " -> 324문자\n",
      "\n",
      "[Summary]    Kerala woman claims husband wanted to sell her as ISIS slave\n",
      " -> 60문자\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _, row in data.sample(2).iterrows():\n",
    "    print(f\"[Text]         {row['Text']}\\n -> {len(row['Text'])}문자\\n\")\n",
    "    print(f\"[Summary]    {row['Summary']}\\n -> {len(row['Summary'])}문자\")\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b108c",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 전처리하기 (추상적 요약)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723acdc0",
   "metadata": {},
   "source": [
    "실습에서 사용된 전처리를 참고하여 각자 필요하다고 생각하는 전처리를 추가 사용하여 텍스트를 정규화 또는 정제해 보세요. 만약, 불용어 제거를 선택한다면 상대적으로 길이가 짧은 요약 데이터에 대해서도 불용어를 제거하는 것이 좋을지 고민해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8e2271",
   "metadata": {},
   "source": [
    "- Abstractive한 문장 요약 결과문이 자연스러운 문장이 되려면 불용어들이 Summary에는 남아 있는 게 더 좋을 것 같음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353def2",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기 (1) 데이터 정리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11953e4a",
   "metadata": {},
   "source": [
    "### 중복 샘플과 NULL 값이 존재하는 샘플 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff53b529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['Text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['Summary'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3796a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "# inplace=True 를 설정하면 DataFrame 타입 값을 return 하지 않고 data 내부를 직접적으로 바꿉니다\n",
    "data.drop_duplicates(subset = ['Text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ce904789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text       0\n",
      "Summary    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f28de01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "#@ 데이터셋의 'Text' 자체에 비어있는 게 없으므로 무의미하지만, 템플릿 느낌으로 포함.\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2799f7f",
   "metadata": {},
   "source": [
    "### 텍스트 정규화와 불용어 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b7570",
   "metadata": {},
   "source": [
    "정규화 사전 출처\n",
    "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "72b6921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "68743692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62e5e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbbcad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Text:    \n",
      "    The formula of Benzene was known long before its structure was determined, as arranging six carbon and six hydrogen atoms without violating the rules of chemistry didn't seem possible. However, German chemist August KekulÃÂ©, while presenting his research on Benzene in 1890, claimed he had discovered its ring structure after having a dream of a snake biting its own tail.\n",
      " => formula benzene known long structure determined arranging six carbon six hydrogen atoms without violating rules chemistry seem possible however german chemist august kekul presenting research benzene claimed discovered ring structure dream snake biting tail\n",
      "=== Summary:    \n",
      "    Idea of Benzene ring structure is based on dream about snake\n",
      " => idea benzene ring structure based dream snake\n"
     ]
    }
   ],
   "source": [
    "# 전처리 후 차이점 살펴보기\n",
    "for _, row in data.sample(1).iterrows():\n",
    "    print(f\"=== Text:    \\n    {row['Text']}\\n => {preprocess_sentence(row['Text'])}\")\n",
    "    print(f\"=== Summary:    \\n    {row['Summary']}\\n => {preprocess_sentence(row['Summary'])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6542949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Text 데이터에 대한 전처리\n",
    "clean_text = []\n",
    "\n",
    "for sentence in data['Text']:\n",
    "    #@ 불용어 제거: 텍스트 길이가 길기 때문\n",
    "    processed_sentence = preprocess_sentence(sentence, remove_stopwords=True)\n",
    "    clean_text.append(processed_sentence)\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 Summary 데이터에 대한 전처리\n",
    "clean_headlines = []\n",
    "\n",
    "for sentence in data['Summary']:\n",
    "    #@ 불용어 제거하지 않음: 텍스트 길이가 짧아서 주요 정보 소실될 수 있음.\n",
    "    processed_sentence = preprocess_sentence(sentence, remove_stopwords=False)  \n",
    "    clean_summary.append(processed_sentence)\n",
    "\n",
    "print(\"Summary 전처리 후 결과: \", clean_headlines[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11124028",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Text'] = clean_text\n",
    "data['Summary'] = clean_summary\n",
    "\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5512430",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18e5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf15484",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기 (2) 훈련데이터와 테스트데이터 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d310c",
   "metadata": {},
   "source": [
    "### 샘플의 최대 길이 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883cdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['Text']]\n",
    "summary_len = [len(s.split()) for s in data['Summary']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Summary')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('Summary')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6dea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: 적당한 길이 설정 필요함 (상의해봐야 함)\n",
    "# text_max_len = 50\n",
    "# summary_max_len = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c964f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "  cnt = 0\n",
    "  for s in nested_list:\n",
    "    if(len(s.split()) <= max_len):\n",
    "        cnt = cnt + 1\n",
    "  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b1e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "below_threshold_len(text_max_len, data['Text'])\n",
    "below_threshold_len(summary_max_len,  data['Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조건에 맞는 샘플만 필터링\n",
    "filtered_data = data[\n",
    "    data['Text'].apply(lambda x: len(x.split()) <= text_max_len) &  # Text 길이 조건\n",
    "    data['Summary'].apply(lambda x: len(x.split()) <= summary_max_len)  # Summary 길이 조건\n",
    "]\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"필터링 전 샘플 수: {len(data)}\")\n",
    "print(f\"필터링 후 샘플 수: {len(filtered_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b93492",
   "metadata": {},
   "source": [
    "### 시작 토큰과 종료 토큰 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['Summary'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['Summary'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['Text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3907e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87aef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6744a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9746c1",
   "metadata": {},
   "source": [
    "## 데이터 전처리하기 (3) 정수 인코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8cf8a",
   "metadata": {},
   "source": [
    "### 단어 집합(vocabulary) 만들기 및 정수 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49706a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(src_tokenizer.word_index.items())[:5])  # 단어-인덱스 맵핑 상위 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c910c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = 8000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 8,000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cabe8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a2469",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_vocab = 2000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73bf763",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53284afb",
   "metadata": {},
   "source": [
    "### 패딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948a5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=summary_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=summary_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=summary_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"encoder_input_train: \", encoder_input_train, \"\\n\", len(encoder_input_train), \"개 (\", len(encoder_input_train[0]), \")\")\n",
    "print(\"encoder_input_test: \", encoder_input_test, \"\\n\", len(encoder_input_test), \"개 (\", len(encoder_input_test[0]), \")\")\n",
    "print(\"decoder_input_train: \", decoder_input_train, \"\\n\", len(decoder_input_train), \"개 (\", len(decoder_input_train[0]), \")\")\n",
    "print(\"decoder_target_train: \", decoder_target_train, \"\\n\", len(decoder_target_train), \"개 (\", len(decoder_target_train[0]), \")\")\n",
    "print(\"decoder_input_test: \", decoder_input_test, \"\\n\", len(decoder_input_test), \"개 (\", len(decoder_input_test[0]), \")\")\n",
    "print(\"decoder_target_test: \", decoder_target_test, \"\\n\", len(decoder_target_test), \"개 (\", len(decoder_target_test[0]), \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bce66a",
   "metadata": {},
   "source": [
    "# Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347e250",
   "metadata": {},
   "source": [
    "일반적인 seq2seq보다는 어텐션 메커니즘을 사용한 seq2seq를 사용하는 것이 더 나은 성능을 얻을 수 있어요. 실습 내용을 참고하여 어텐션 메커니즘을 사용한 seq2seq를 설계해 보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2affc3d",
   "metadata": {},
   "source": [
    "## Seq2Seq (기본) 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "encoder_lstm3 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4)\n",
    "encoder_output3, state_h3, state_c3 = encoder_lstm3(encoder_output2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896d0dd6",
   "metadata": {},
   "source": [
    "## 어텐션 메커니즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e17ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5ef73b",
   "metadata": {},
   "source": [
    "## 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192feded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a694b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4983be7",
   "metadata": {},
   "source": [
    "## 인퍼런스 모델 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1e338",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: 이거 구현을 해야 모델 성능 테스트가 가능한 건가? 그럼 스텝 3인가? 4인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a945789",
   "metadata": {},
   "source": [
    "# Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000c38d",
   "metadata": {},
   "source": [
    "원래의 요약문(headlines 열 == Summary 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4666eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@TODO: 뭘 어떻게 비교할까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e3ecd",
   "metadata": {},
   "source": [
    "# Step 5. Summa을 이용해서 추출적 요약해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695ab0c",
   "metadata": {},
   "source": [
    "추상적 요약은 추출적 요약과는 달리 문장의 표현력을 다양하게 가져갈 수 있지만, 추출적 요약에 비해서 난이도가 높아요. 반대로 말하면 추출적 요약은 추상적 요약에 비해 난이도가 낮고 기존 문장에서 문장을 꺼내오는 것이므로 잘못된 요약이 나올 가능성이 낮아요.\n",
    "\n",
    "Summa의 summarize를 사용하여 추출적 요약을 해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68c75d",
   "metadata": {},
   "source": [
    "## 데이터 다운로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb34adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1be5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get('http://rare-technologies.com/the_matrix_synopsis.txt').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a25884",
   "metadata": {},
   "source": [
    "## summarize 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb2a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, ratio=0.005, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95e17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Summary:')\n",
    "print(summarize(text, words=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0efa27d",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f0f544",
   "metadata": {},
   "source": [
    "마크다운 파일로 따로 제작하였음."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
